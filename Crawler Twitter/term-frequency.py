import operator 
import json
import processing
import graph
from collections import Counter
from nltk.corpus import stopwords
import string
from nltk import bigrams 

dicionario = ['rt', 'via', 'RT', '‚Ä¶', ';', 'a', 'ou', "",
            'aff', ',', ':', '√©', '√ß√£o', '√ßa', 'A', '√£o', 'O', 'popula', 'est', 'come', '√°s',
            'v√£o', 'pr√©', 'pre', 'pol', '√ß√µes', '√°', '√ßos', '√ßo', 'q', 's√°', 'd', 'E', 'e', 'pra',
            '√©m', 'üá∑', 'üáß', 'n√£o', 'ser', 'N√£o', '30', 'boa', '√™ncia', 'vai']

fname = 'cpidacovid.json'

with open(fname, 'r') as f:
    count_all = Counter()
    for line in f:
        tweet = json.loads(line)
        terms_all = [term for term in processing.preprocess(tweet['text'])]


        punctuation = list(string.punctuation)
        stop = stopwords.words('portuguese') + punctuation + dicionario

        terms_stop = [term for term in terms_all if term not in stop]

        # Count terms only once, equivalent to Document Frequency
        terms_single = set(terms_all)

        # Count hashtags only
        terms_hash = [term for term in processing.preprocess(tweet['text']) 
                    if term.startswith('#')]

        # Count terms only (no hashtags, no mentions)
        terms_only = [term for term in terms_stop if term not in stop and 
                        not term.startswith(('#', '@'))] 
                    # mind the ((double brackets))
                    # startswith() takes a tuple (not a list) if 
                    # we pass a list of inputs

        
        count_all.update(terms_only)
    
    common = (count_all.most_common(20))

    #common.remove(('Ô∏è', 24))

    cinco_maior = graph.cinco_maior(common)    
    lista_cinco = graph.lista_cinco_maior(common, cinco_maior)
    print(common)
    graph.plotgraph(common, lista_cinco)